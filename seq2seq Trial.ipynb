{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_summary = []\n",
    "source_tweets = []\n",
    "f = open('./MainDataset/tweet_dataset/parallel_data/summaries')\n",
    "for line in f:\n",
    "    source_summary.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()\n",
    "f = open('./MainDataset/tweet_dataset/parallel_data/tweets')\n",
    "for line in f:\n",
    "    source_tweets.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will try to train on only 10000 tweets for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_summary = source_summary[:10000]\n",
    "source_tweets = source_tweets[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the lookup tables for the source data and source tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODES = {'<PAD>':0,'<EOS>':1,'<UNK>':2,'<GO>':3}\n",
    "def createLookupTables(text):\n",
    "    vocab = set(text.split())\n",
    "    vocab_to_int = copy.copy(CODES)\n",
    "    for v_i, v in enumerate(vocab,len(CODES)):\n",
    "        vocab_to_int[v] = v_i\n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that uses the source data vocab_to_int and source tweets vocab_to_int\n",
    "# And converts them to their respective vocab ids\n",
    "def text_to_ids(source_sentences,target_sentences,source_vocab_to_int,target_vocab_to_int):\n",
    "    source_text_id = []\n",
    "    target_text_id = []\n",
    "    for i in xrange(len(source_sentences)):\n",
    "        source_sentence = source_sentences[i]\n",
    "        target_sentence = target_sentences[i]\n",
    "        source_tokens = source_sentence.split()\n",
    "        target_tokens = target_sentence.split()\n",
    "        source_token_id = []\n",
    "        target_token_id = []\n",
    "        for index, token in enumerate(source_tokens):\n",
    "            if token != \"\":\n",
    "                source_token_id.append(source_vocab_to_int[token])\n",
    "        for index, token in enumerate(target_tokens):\n",
    "            if token != \"\":\n",
    "                target_token_id.append(target_vocab_to_int[token])\n",
    "        target_token_id.append(target_vocab_to_int['<EOS>'])\n",
    "        source_text_id.append(source_token_id)\n",
    "        target_text_id.append(target_token_id)\n",
    "    return source_text_id, target_text_id\n",
    "\n",
    "def preprocess_and_save_data(source_sentences,target_sentences,text_to_ids):\n",
    "    source_text = \"\\n\".join(source_sentences)\n",
    "    target_text = \"\\n\".join(target_sentences)\n",
    "    source_text = source_text.lower()\n",
    "    target_text = target_text.lower()\n",
    "    source_sentences = source_text.split(\"\\n\")\n",
    "    target_sentences = target_text.split(\"\\n\")\n",
    "    source_vocab_to_int, source_int_to_vocab = createLookupTables(source_text)\n",
    "    target_vocab_to_int, target_int_to_vocab = createLookupTables(target_text)\n",
    "    source_text, target_text = text_to_ids(source_sentences,target_sentences,source_vocab_to_int,target_vocab_to_int)\n",
    "    pickle.dump(((source_text, target_text),(source_vocab_to_int, target_vocab_to_int),(source_int_to_vocab, target_int_to_vocab)), open('preprocess.p', 'wb'))\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_save_data(source_summary,source_tweets,text_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessed file saved in a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_preprocess():\n",
    "    with open('preprocess.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU Found! All the Best for training.\n",
      "Tensorflow Version: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "if not tf.test.gpu_device_name():\n",
    "    print \"No GPU Found! All the Best for training.\"\n",
    "print \"Tensorflow Version:\",tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_dec_model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n",
    "    \n",
    "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
    "    max_target_len = tf.reduce_max(target_sequence_length)    \n",
    "    \n",
    "    return inputs, targets, target_sequence_length, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# hyperparam_inputs function creates and returns parameters (TF placeholders) related to hyper-parameters to the model.\n",
    "\n",
    "#     lr_rate is learning rate\n",
    "#     keep_prob is the keep probability for Dropouts\n",
    "\n",
    "\n",
    "\n",
    "def hyperparam_inputs():\n",
    "    lr_rate = tf.placeholder(tf.float32, name='lr_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return lr_rate, keep_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for encoding\n",
    "    :return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    # get '<GO>' id\n",
    "    go_id = target_vocab_to_int['<GO>']\n",
    "    \n",
    "    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n",
    "    \n",
    "    return after_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    \"\"\"\n",
    "    :return: tuple (RNN output, RNN state)\n",
    "    \"\"\"\n",
    "    embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n",
    "                                             vocab_size=source_vocab_size, \n",
    "                                             embed_dim=encoding_embedding_size)\n",
    "    \n",
    "    stacked_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(rnn_size), keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    outputs, state = tf.nn.dynamic_rnn(stacked_cells, \n",
    "                                       embed, \n",
    "                                       dtype=tf.float32)\n",
    "    return outputs, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a training process in decoding layer \n",
    "    :return: BasicDecoderOutput containing training logits and sample_id\n",
    "    \"\"\"\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "    \n",
    "    # for only input layer\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, \n",
    "                                               target_sequence_length)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "\n",
    "    # unrolling the decoder layer\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_summary_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a inference process in decoding layer \n",
    "    :return: BasicDecoderOutput containing inference logits and sample_id\n",
    "    \"\"\"\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
    "                                                      tf.fill([batch_size], start_of_sequence_id), \n",
    "                                                      end_of_sequence_id)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
    "                                              helper, \n",
    "                                              encoder_state, \n",
    "                                              output_layer)\n",
    "    \n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_target_sequence_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_input, encoder_state,\n",
    "                   target_sequence_length, max_target_sequence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    target_vocab_size = len(target_vocab_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        output_layer = tf.layers.Dense(target_vocab_size)\n",
    "        train_output = decoding_layer_train(encoder_state, \n",
    "                                            cells, \n",
    "                                            dec_embed_input, \n",
    "                                            target_sequence_length, \n",
    "                                            max_target_sequence_length, \n",
    "                                            output_layer, \n",
    "                                            keep_prob)\n",
    "\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        infer_output = decoding_layer_infer(encoder_state, \n",
    "                                            cells, \n",
    "                                            dec_embeddings, \n",
    "                                            target_vocab_to_int['<GO>'], \n",
    "                                            target_vocab_to_int['<EOS>'], \n",
    "                                            max_target_sequence_length, \n",
    "                                            target_vocab_size, \n",
    "                                            output_layer,\n",
    "                                            batch_size,\n",
    "                                            keep_prob)\n",
    "\n",
    "    return (train_output, infer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence model\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    enc_outputs, enc_states = encoding_layer(input_data, \n",
    "                                             rnn_size, \n",
    "                                             num_layers, \n",
    "                                             keep_prob, \n",
    "                                             source_vocab_size, \n",
    "                                             enc_embedding_size)\n",
    "    \n",
    "    dec_input = process_decoder_input(target_data, \n",
    "                                      target_vocab_to_int, \n",
    "                                      batch_size)\n",
    "    \n",
    "    train_output, infer_output = decoding_layer(dec_input,\n",
    "                                               enc_states, \n",
    "                                               target_sequence_length, \n",
    "                                               max_target_sentence_length,\n",
    "                                               rnn_size,\n",
    "                                              num_layers,\n",
    "                                              target_vocab_to_int,\n",
    "                                              target_vocab_size,\n",
    "                                              batch_size,\n",
    "                                              keep_prob,\n",
    "                                              dec_embedding_size)\n",
    "    \n",
    "    return train_output, infer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_step = 300\n",
    "\n",
    "epochs = 13\n",
    "batch_size = 128\n",
    "\n",
    "rnn_size = 128\n",
    "num_layers = 3\n",
    "\n",
    "encoding_embedding_size = 200\n",
    "decoding_embedding_size = 200\n",
    "\n",
    "learning_rate = 0.001\n",
    "keep_probability = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'checkpoints/dev'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\n",
    "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, target_sequence_length, max_target_sequence_length = enc_dec_model_inputs()\n",
    "    lr, keep_prob = hyperparam_inputs()\n",
    "    \n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    "    \n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n",
    "    # - Returns a mask tensor representing the first N positions of each cell.\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function - weighted softmax cross entropy\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "\n",
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Split Done!\n",
      "Validation Split Done!\n",
      "Session Created?!\n",
      "Epoch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Epoch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Epoch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Epoch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Epoch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Epoch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Epoch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Epoch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Epoch Train!\n",
      "Batch Train!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Epoch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Epoch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Epoch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Epoch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Batch Train!\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))\n",
    "\n",
    "# Split data to training and validation sets\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "print \"Train Split Done!\"\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "print \"Validation Split Done!\"\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_vocab_to_int['<PAD>'],\n",
    "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    print \"Session Created?!\"\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        print \"Epoch Train!\"\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "            print \"Batch Train!\"\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print \"Here 1\"\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "                print \"Here 2\"\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "                print \"Training Accuracy Found\"\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "                print \"Validation Accuracy Found\"\n",
    "\n",
    "                print \"Epoch\",epoch_i,\"Batch\",batch_i,len(source_int_text) // batch_size,\"Train Accuracy\",train_acc,\"Validation Accuracy\",valid_acc,\"Loss\",loss\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_params(params):\n",
    "    with open('params.p', 'wb') as out_file:\n",
    "        pickle.dump(params, out_file)\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    with open('params.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()\n",
    "load_path = load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
      "Input\n",
      "  Word Ids:      [2, 40634, 27620, 575, 41510, 12533, 23956, 25370, 14007, 40566, 33150, 14043, 22116, 9588, 22541, 15869, 26710, 12452, 25401, 43955, 5313, 2, 12215, 22116, 40070, 45564, 11186, 15405, 24088, 22197, 40566, 25909, 15943, 14141, 17141, 25370, 42866, 12452, 15886, 26676, 30036, 39968, 47585, 12539, 12452, 40566, 25401, 43955, 40634, 24483, 2, 40070, 1814, 40566, 25625, 29514, 22116, 22197, 40634, 5314, 37304, 39968, 15405, 40634, 26676, 40070, 12165, 28066, 12452, 15405, 14805, 22197, 40566, 19739, 40004, 5876, 40634, 13531, 2, 40634, 41520, 40070, 15085, 35332, 24559, 11065, 12452, 15405, 32096, 22197, 40566, 16594, 25909, 1608, 40634, 20966, 2, 13647, 22197, 15556, 37162, 305, 14029, 15405, 40634, 26676, 21875, 15405, 40477, 22197, 36712, 17141, 25370, 8199, 13313, 2205, 40070, 38081, 14029, 15405, 15189, 18417, 35554, 38239, 5876, 40634, 41520, 40070, 24558, 2, 38239, 1668, 27341, 7345, 15405, 2906, 22197, 45479, 39598, 18417, 15405, 12215, 38766, 21875, 3916, 15405, 17141, 38165, 39968, 47585, 14007, 305, 9588, 69, 40566, 8725, 26824, 25247, 25370, 19151, 21875, 8199, 40566, 25401, 43955, 2205, 22194, 40566, 12215, 40634, 31676, 2]\n",
      "  English Words: ['<UNK>', 'quantum', 'computing', 'has', 'recently', 'proven', 'itself', 'to', 'be', 'a', 'powerful', 'computational', 'model', 'when', 'constructing', 'viable', 'architectures', 'for', 'large', 'scale', 'computation.', '<UNK>', 'topological', 'model', 'is', 'constructed', 'from', 'the', 'foundation', 'of', 'a', 'error', 'correction', 'code,', 'required', 'to', 'correct', 'for', 'inevitable', 'hardware', 'faults', 'that', 'will', 'exist', 'for', 'a', 'large', 'scale', 'quantum', 'device.', '<UNK>', 'is', 'also', 'a', 'measurement', 'based', 'model', 'of', 'quantum', 'computation,', 'meaning', 'that', 'the', 'quantum', 'hardware', 'is', 'responsible', 'only', 'for', 'the', 'construction', 'of', 'a', 'large,', 'computationally', 'universal', 'quantum', 'state.', '<UNK>', 'quantum', 'state', 'is', 'then', 'strategically', 'consumed,', 'allowing', 'for', 'the', 'realisation', 'of', 'a', 'fully', 'error', 'corrected', 'quantum', 'algorithm.', '<UNK>', 'number', 'of', 'physical', 'qubits', 'needed', 'by', 'the', 'quantum', 'hardware', 'and', 'the', 'amount', 'of', 'time', 'required', 'to', 'implement', 'an', 'algorithm', 'is', 'dictated', 'by', 'the', 'manner', 'in', 'which', 'this', 'universal', 'quantum', 'state', 'is', 'consumed.', '<UNK>', 'this', 'paper', 'we', 'examine', 'the', 'problem', 'of', 'algorithmic', 'optimisation', 'in', 'the', 'topological', 'lattice', 'and', 'introduce', 'the', 'required', 'elements', 'that', 'will', 'be', 'needed', 'when', 'designing', 'a', 'classical', 'software', 'package', 'to', 'compile', 'and', 'implement', 'a', 'large', 'scale', 'algorithm', 'on', 'a', 'topological', 'quantum', 'computer.', '<UNK>']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [326, 14530, 14832, 5025, 5025, 5025, 5025, 5025, 5025, 5025, 5025, 5025, 5025, 5025, 7209, 1]\n",
      "  French Words: rt @mathpaper: of the the the the the the the the the the the ... <EOS>\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    results = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in vocab_to_int:\n",
    "            results.append(vocab_to_int[word])\n",
    "        else:\n",
    "            results.append(vocab_to_int['<UNK>'])\n",
    "            \n",
    "    return results\n",
    "\n",
    "translate_sentence = 'Topological quantum computing has recently proven itself to be a powerful computational model when constructing viable architectures for large scale computation. The topological model is constructed from the foundation of a error correction code, required to correct for inevitable hardware faults that will exist for a large scale quantum device. It is also a measurement based model of quantum computation, meaning that the quantum hardware is responsible only for the construction of a large, computationally universal quantum state. This quantum state is then strategically consumed, allowing for the realisation of a fully error corrected quantum algorithm. The number of physical qubits needed by the quantum hardware and the amount of time required to implement an algorithm is dictated by the manner in which this universal quantum state is consumed. In this paper we examine the problem of algorithmic optimisation in the topological lattice and introduce the required elements that will be needed when designing a classical software package to compile and implement a large scale algorithm on a topological quantum computer. '\n",
    "\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
    "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "print('  French Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
